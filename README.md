# Distributed Cholesky

**Name**: Cholesky  
**Contact Person**: OmpSs@FPGA Team, ompss-fpga-support@bsc.es  
**License Agreement**: GPL 3.0  
**Platform**: OmpSs@FPGA+IMP+OMPIF

## Description

The Cholesky benchmark decomposes a Hermitian positive-definite square symmetric matrix into a lower-triangular matrix that solves the following equation:
$$A = LL^T$$
$A$ is the input matrix, $L$ is the output.
The decomposed matrix $L$ multiplied by its transposed $L^T$ results in the original input $A$.
This implementation uses single precision floating-point to represent the matrix elements, stored in column-major order.
There are two reason why it uses column-major.
First, we use the `potrf`from the LAPACK library which is implemented in Fortran, thus it expects column-major.
Second, this layout is useful for the FPGA implementation as we can optimize better the code, more details in the respective section.

### Parallelization with tasks

To parallelize Cholesky using tasks, we distribute the matrix into multiple square blocks, and each task operates at the block level.
There are 4 kernels in total:
```C++
static const int BS = ...;

#pragma oss task inout(A)
void potrf(float A[BS][BS]);

#pragma oss task input(A) inout(B)
void trsm(floag A[BS][BS], float B[BS][BS]);

#pragma oss task input(A) input(B) inout(C)
void gemm(float A[BS][BS], float B[BS][BS], float C[BS][BS]);

#pragma oss task input(A) inout(B)
void syrk(float A[BS][BS], float B[BS][BS]);

void cholesky_blocked(const int nt, float *A[nt][nt])
{
   for (int k = 0; k < nt; k++) {
      potrf( A[k][k] );
      for (int i = k+1; i < nt; i++) {
         trsm( A[k][k],
               A[k][i] );
      }
      for (int i = k + 1; i < nt; i++) {
         for (int j = k + 1; j < i; j++) {
            gemm( A[k][i],
                  A[k][j],
                  A[j][i] );
         }
         syrk( A[k][i],
               A[i][i] );
      }
   }
}
```

You will see that this code is not exactly copied from the `cholesky.c` file, it is simplified to make it more readable and easier to understand.
`cholesky_blocked` takes as input two parameters, the number of tiles (blocks) `nt` and an array of pointers `A`.
All kernels have as parameters two-dimensional arrays of a fixed size `BS` which is the block size.
Therefore, they expect the block input to be consecutive in memory.
Array `A` is a matrix of pointers to the blocks.
`potrf` is the Cholesky decomposition of a single block, defined in the LAPACK library.
The others, `trsm`, `gemm` and `syrk` are defined in he BLAS spec.
Short description found in BLAS webpage:
* `trsm`: solving triangular matrix with multiple right hand sides
* `gemm`: matrix matrix multiply
* `syrk`: symmetric rank-k update to a matrix

The following animation shows on the right the task graph generated by a 4x4 block matrix (any block size).
On the left, the 4x4 block matrix, the colored block is the output (the `inout` parameter), and the grey highlited block are the inputs (the `input` parameters).

![test](https://github.com/bsc-pm-ompss-at-fpga/distributed_Cholesky/assets/17345627/574a499b-5a01-4be8-af61-393bf13b31a0)

And here is the final task graph:

![cholesky_animation](https://github.com/bsc-pm-ompss-at-fpga/distributed_Cholesky/assets/17345627/3a9d5861-ea18-4236-ab6c-85a07256479e)

### Parallelization with IMP

The idea is simple: assign each block to a rank, and the task is executed by the rank that has the `inout` block assigned to it.
If there are input blocks assigned to other ranks, IMP will take care of moving data.
However, in this benchmark there is a problem with this straight-forward implementation that comes when a single block is copied multiple times on the same rank.
There is a cache implementation of IMP that memorizes previous copies between blocks, and optimizes the ones that hit in the cache.
The amount of optimization depends on several cache parameters, like size and replacement policy.
In this repository there is another implementation that optimizes all copies in the user code.
The reasoning behind these optimizations are explained in a later section.
Asuming that a block is only sent once, the following image shows the task graph of a 3x3 block matrix, and the same task graph when distributing it in two ranks.

![cholesky_imp (1)](https://github.com/bsc-pm-ompss-at-fpga/distributed_Cholesky/assets/17345627/bf45b536-0262-4e15-bbac-b43ac0fe65e3)

The rightmost figure is the sequence of sends/receives between rank 0 (left column of matrices) and 1 (right column of matrices).
The matrix numbers indicate the rank assigned to each block, and the blocks present in both ranks.
We use a cyclic distribution for both rows and columns, which takes the formula $(i+j)\\%n$ where $n$ is the number of ranks, and $i,j$ the block matrix coordinates.
After trying other distributions, this one seems to get better performance by balancing the workload of every rank.
At the top of the figure, we have the rank assignment for every block.
After that, the sequence of sends and receives in the same order they appear in the middle graph.
All ranks have the same matrix allocated, though they only have the contents of the blocks assigned to them.
Blank positions mean there is no valid data in that block.
In the first step, we can see the block $(0,0)$ moves from rank 0 to rank 1 which was initially blank.
Rank 1 needs the contents of that block because it is an input dependence of the `trsm` task (labeled 1 in the graphs) at block $(1,0)$.
The block generated by the `trsm` of rank 1 is needed later by a `syrk` task on rank 0 at block $(1,1)$ (third row in the send/recv sequence).
The same logic applies for the rest of send/recv pairs of the example.

#### Copy optimizations

As we explained before, this implementation optimizes copies in the user code.
It is easier to do this way because the user has the knowledge of how the tasks are created, and how the blocks are distributed in memory.
The latter is essential to get the optimal number of send/receives.
Lets go in order for every possible copy on each kernel:

##### POTRF

There are no input blocks, so we will never have communication with this kernel.

##### TRSM

There is one input block, $(k,k)$, which is the block produced by a `potrf` task, and one output block $(i,k)$ (the code presented at the beggining has these coordinates swapped because of the column-major order).
We know that $i$ is increased by one on each iteration of the `trsm` loop, and since $k$ is constant for this loop, blocks of consecutive iterations are assigned to consecutive ranks.
If the number of blocks in one dimension is greater than the number of ranks, we know that the same rank will execute at least two tasks with the same $(k,k)$ block.
We can optimize the second and any other copy of this block, by only doing IMP when $i-(k+1) < n$ where $n$ is the number of ranks.
I.e. if this condition is true, we give one data owner with the $(k,k)$ block.
If not, we give 0 data owners, ensuring that there's no communication.
Here is a visual example with 3 ranks on a 6x6 block matrix (only showing the first column):

![cholesky_imp (2)](https://github.com/bsc-pm-ompss-at-fpga/distributed_Cholesky/assets/17345627/1084518a-a0a5-44b1-b631-a13421fda697)

The numbers on each position of the column represent the rank assigned to that block.
$i-(k+1)$ is the iteration count starting from 0.
It tells how many `trsm` tasks have already been created.
We can see that rank 1 repeats after creating 3 tasks, because there are 3 ranks and the distribution is cyclic.
This same reasoning applies to every rank, so we know that after creating $n$ tasks, whichever is the rank that has the $(k,k)$ block, the loop will have already created tasks for each rank.
Thus, every rank has the $(k,k)$, and we don't have to repeat the copy.

##### GEMM

This one is a little more tricky, but we use the same reasoning than before.
In fact, all optimizations are based on the fact that the same block is sent to consecutive ranks, so after $n$ tasks of that block, we can stop communication.
There are two input blocks, always produced by previous `trsm` tasks, $(i,k)$ and $(j,k)$, and one output block $(i,j)$.

First, lets look at the first input dependence over block $(i,k)$, which is the easiest to understand.
This is identical to the `trsm` case, but loop variable $j$ traverses rows instead of columns.
Block $(i,k)$ is constant for the `gemm` loop, since it modifies variable $j$, used in the output block $(i,j)$.
Thus, after $n$ iterations, the block $(i,k)$ is already sent to every rank.
We can see this in the following image:

![cholesky_imp (2)](https://github.com/bsc-pm-ompss-at-fpga/distributed_Cholesky/assets/17345627/3245d043-d5a9-4c7b-a84b-8bea6dc4d157)

Its the same case, but there is an extra variable $j$.
Also, we can be sure that block $(i,k)$ is not sent before the `gemm` loop, because for the same $k$, $i$ only increases and thus never repeats the same row.
The $k$ variable also increases without repeating, so on different $k$ the input block also changes and thus we never repeat.
In conclusion, the condition to enable communication is $j-(k+1) < n$.

The second dependence is over block $(j,k)$, which changes on every iteration.
However, the condition ends being $i-(k+1) < n$.
This is because the value $i-(k+1)$ tells us how many tasks on previous $i$ iterations have the block $(i,k)$ as input.
We can also see it as the distance between the input and output blocks according to the cyclic formula $(i+j)\\%n$.
These tasks include `gemm` and `syrk`.
The value of the condition is constant for the $j$ loop because the distance between the input and output blocks is also constant.

![cholesky_imp](https://github.com/bsc-pm-ompss-at-fpga/distributed_Cholesky/assets/17345627/cd869471-c07d-4e93-a8ac-d16a283ac048)

Here we see all the tasks that have as input the same block of the second dependence of a `gemm` task for a 4x4 block matrix with 4 ranks (showing only the relevant blocks).
First, that block is the first dependence of the `gemm` tasks for $i-1$ (if any), as well as the input block for the `syrk` task of $i-1$.
After that, the block $(j,k)$ appears as the second dependence of all `gemm` with the same $j$.
The number of tasks created since the first appearance is in fact $i-(k+1)$, and although the coordinates travel on the two dimensions, each task is assigned to consecutive ranks because the assignment is cyclic on both dimensions.
I.e. after $n$ tasks, the ranks start repeating, hence the condition we introduced earlier $i-(k+1) < n$.

In summary, since we know that $i > j$ for all $i$, we can combine both conditions to decide the number of data owners of a `gemm` task.
* If $i-(k+1) < n$, we know that $j-(k+1) < n$, so both dependencies may communicate (2 data owners).
* If $i-(k+1) >= n$ and $j-(k+1) < n$, only the first dependence may communicate (1 data owner).
* if $i-(k+1) >= n$ and $j-(k+1) >= n$ there's no communication (0 data owners).

##### SYRK

This one is as easy as `trsm`.
`syrk` has one input block $(i,k)$ and one output block $(i,i)$.
Before a `syrk` task on row $i$ of any $k$, there are $i-(k+1)$ `gemm` tasks with the same $(i,k)$ input block, so the condition is the same $i-(k+1) < n$.
You can see that in the last figure, but for completeness here is an example of a 5x5 block matrix with 3 ranks.

![cholesky_imp (1)](https://github.com/bsc-pm-ompss-at-fpga/distributed_Cholesky/assets/17345627/fdd4b5aa-2258-4337-af64-cad3451b8d98)


### Memory optimization

As you have seen in the last figure and in the code, Cholesky only reads and writes the lower triangle of the matrix because this one is symmetric.
Since the matrix dimensions must be square, the size increases very fast, and with this implementation we allocate the whole matrix on every rank.
A small optimization that we do is only allocate the useful part, i.e. the lower triangle of the block matrix.
This is not exactly the lower triangle of the whole matrix, because on the blocks of the diagonal there are elements of the upper triangle, but we save close to 2x memory by doing this.

![cholesky_imp (4)](https://github.com/bsc-pm-ompss-at-fpga/distributed_Cholesky/assets/17345627/7c717c5c-12b7-4d84-b2c0-3ea1f57fea93)

This image show how we store the matrix in memory.
First there is the original matrix, the numbers are coordinates to each element, and the colors represent the block each element belongs to.
Just below that, the linear representation in memory stored in column-major order.
Then, we can see the three blocks that are actually used by Cholesky, thus, we only store that part in memory.
Besides only storing the needed blocks, these are stored consecutive in memory in column-major order for elements in a block, and for different blocks.
This is needed because as we explained earlier, each kernel expects to have the block consecutive in memory.
Also, loading an entire block from memory is faster because we can do a single copy instead of doing one per row.

## FPGA implementation

### POTRF

The `potrf` kernel FPGA implementation is not very interesing.
It doesn't give much margin for optimizations due to the access patterns and dependencies between iterations.
There are pipelining directives, but the initiation interval can't reach 1.
To give a little boost, the loops are manually unrolled by a factor of 2.
However, the total execution time of `potrf` compared to the rest is very small so we didn't dedicate much efforts in optimizing the kernel.

### TRSM

The `trsm` solves a triangular magtrix equation, storing the result in block $B$, and $A$ is transposed.
There is a main loop over $k$.
Inside, first an $i$ loop that is pipelined and unrolled by a configurable factor (in the code is unrolled to be executed in 2 iterations).
Here we get II 1 because the $B$ block is partitioned cyclically by columns (since is column-major), and the loop iterates over consecutive columns.
In this case, column-major order helps because if it were row major, we would have to change partitioning and thus slow-down loading the block from memory, as well as storing the result.
The code loads the block as fast as possible with a data bus that can bring multiple data elements per cycle, but for that we need a cylic partitioning so we can store the data in BRAMs in the same cycle.
With a column partition, positions from consecutive column would be stored in the same memory bank, and it would not be possible to store them in parallel.

The second loop is a nested loop.
It is pipelined with a factor of 2 (it could be 1 but this way we reduce resource usage of this kernel), and the inner loop is also unrolled implicitly by the pipeline directive.
Since `tmp_row` and $B$ (it's called row but in fact it's a column) are partitioned, we can access to half of the elements in parallel to achieve II=2.

### GEMM



